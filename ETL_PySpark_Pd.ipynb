{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1nm9qK7FQeyZGqLMIRUZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvc2010/ETL/blob/main/ETL_PySpark_Pd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy3OBlHKoyS6"
      },
      "outputs": [],
      "source": [
        "# 1. Instalação de Bibliotecas:\n",
        "# Instale PySpark e openpyxl no Google Colab para configurar o ambiente.\n",
        "!pip install pyspark\n",
        "!pip install openpyxl\n",
        "\n",
        "# 2. Criação da Sessão Spark:\n",
        "# Importe as bibliotecas necessárias e crie uma sessão Spark para usar as funcionalidades do PySpark.\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exemplo ETL\").getOrCreate()\n",
        "\n",
        "# 3. Extração: Carregar o Arquivo Excel com Pandas:\n",
        "# Carregue o arquivo Excel utilizando a biblioteca Pandas.\n",
        "df_excel = pd.read_excel('data_faturamento.xlsx')\n",
        "\n",
        "# 4. Conversão para CSV:\n",
        "# Converta o DataFrame lido com Pandas para um arquivo CSV.\n",
        "df_excel.to_csv('data_faturamento.csv', index=False)\n",
        "\n",
        "# 5. Carregar o CSV usando PySpark:\n",
        "# Leia o arquivo CSV usando PySpark e exiba os dados carregados.\n",
        "df_spark = spark.read.csv('data_faturamento.csv', header=True, inferSchema=True)\n",
        "df_spark.show()\n",
        "df_spark.printSchema()\n",
        "\n",
        "# 6. Transformação: Filtragem e Adição de Coluna:\n",
        "# Aplique transformações no DataFrame PySpark, filtrando registros e adicionando uma nova coluna.\n",
        "df_transformed = df_spark.filter(col(\"faturamento\") > 1000)\n",
        "df_transformed = df_transformed.withColumn(\"ValorComTaxa\", col(\"faturamento\") * 1.1)\n",
        "df_transformed.show()\n",
        "\n",
        "# 7. Salvar Dados Transformados em Novo CSV:\n",
        "# Salve os dados transformados em um novo arquivo CSV.\n",
        "df_transformed.write.csv('data_faturamento_transformado.csv', header=True)\n",
        "\n",
        "# 8. Converter PySpark DataFrame para Pandas:\n",
        "# Converta o DataFrame PySpark transformado para um DataFrame Pandas para facilitar a manipulação de dados.\n",
        "df_transformed_pd = df_transformed.toPandas()\n",
        "\n",
        "# 9. Gerar e Salvar Comandos SQL:\n",
        "# Crie comandos SQL de inserção com base nos dados transformados e salve-os em um arquivo SQL.\n",
        "table_name = 'data_faturamento'\n",
        "sql_inserts = []\n",
        "\n",
        "for index, row in df_transformed_pd.iterrows():\n",
        "    values = \"', '\".join([str(x).replace(\"'\", \"''\") for x in row.values])\n",
        "    sql_inserts.append(f\"INSERT INTO {table_name} VALUES ('{values}');\")\n",
        "\n",
        "with open('data_faturamento_transformado.sql', 'w') as file:\n",
        "    file.write('\\n'.join(sql_inserts))\n",
        "\n",
        "# 10. Encerrar a Sessão Spark:\n",
        "# Encerre a sessão Spark para liberar recursos.\n",
        "spark.stop()\n"
      ]
    }
  ]
}